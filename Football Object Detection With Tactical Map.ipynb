{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football Object Detection with Tactical Map Position Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import skimage\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to get the labels as defined for the two differents datasets, we have them stored in these YAML configuration files that were used to train the YOLO models.\n",
    "Now, we have both numerical and alphabetical representation of the labels, and we will be using that to identify the objects in the frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known coordinates of each keypoint on the tactical map:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TLC</th>\n",
       "      <th>TRC</th>\n",
       "      <th>TR6MC</th>\n",
       "      <th>TL6MC</th>\n",
       "      <th>TR6ML</th>\n",
       "      <th>TL6ML</th>\n",
       "      <th>TR18MC</th>\n",
       "      <th>TL18MC</th>\n",
       "      <th>TR18ML</th>\n",
       "      <th>TL18ML</th>\n",
       "      <th>...</th>\n",
       "      <th>BR6MC</th>\n",
       "      <th>BL6MC</th>\n",
       "      <th>BR6ML</th>\n",
       "      <th>BL6ML</th>\n",
       "      <th>BR18MC</th>\n",
       "      <th>BL18MC</th>\n",
       "      <th>BR18ML</th>\n",
       "      <th>BL18ML</th>\n",
       "      <th>BRArc</th>\n",
       "      <th>BLArc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>15</td>\n",
       "      <td>291</td>\n",
       "      <td>188</td>\n",
       "      <td>116</td>\n",
       "      <td>189</td>\n",
       "      <td>116</td>\n",
       "      <td>221</td>\n",
       "      <td>84</td>\n",
       "      <td>221</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>117</td>\n",
       "      <td>189</td>\n",
       "      <td>117</td>\n",
       "      <td>221</td>\n",
       "      <td>84</td>\n",
       "      <td>221</td>\n",
       "      <td>84</td>\n",
       "      <td>182</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>521</td>\n",
       "      <td>521</td>\n",
       "      <td>463</td>\n",
       "      <td>463</td>\n",
       "      <td>521</td>\n",
       "      <td>521</td>\n",
       "      <td>463</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TLC  TRC  TR6MC  TL6MC  TR6ML  TL6ML  TR18MC  TL18MC  TR18ML  TL18ML  ...  \\\n",
       "x   15  291    188    116    189    116     221      84     221      84  ...   \n",
       "y   15   15     35     35     15     15      73      73      15      15  ...   \n",
       "\n",
       "   BR6MC  BL6MC  BR6ML  BL6ML  BR18MC  BL18MC  BR18ML  BL18ML  BRArc  BLArc  \n",
       "x    189    117    189    117     221      84     221      84    182    121  \n",
       "y    501    501    521    521     463     463     521     521    463    463  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical label of field keypoints (as defined when training the Yolo model):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>alpha_label</th>\n",
       "      <th>TLC</th>\n",
       "      <th>TRC</th>\n",
       "      <th>TR6MC</th>\n",
       "      <th>TL6MC</th>\n",
       "      <th>TR6ML</th>\n",
       "      <th>TL6ML</th>\n",
       "      <th>TR18MC</th>\n",
       "      <th>TL18MC</th>\n",
       "      <th>TR18ML</th>\n",
       "      <th>TL18ML</th>\n",
       "      <th>...</th>\n",
       "      <th>BR6MC</th>\n",
       "      <th>BL6MC</th>\n",
       "      <th>BR6ML</th>\n",
       "      <th>BL6ML</th>\n",
       "      <th>BR18MC</th>\n",
       "      <th>BL18MC</th>\n",
       "      <th>BR18ML</th>\n",
       "      <th>BL18ML</th>\n",
       "      <th>BRArc</th>\n",
       "      <th>BLArc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_label</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "alpha_label  TLC  TRC  TR6MC  TL6MC  TR6ML  TL6ML  TR18MC  TL18MC  TR18ML  \\\n",
       "num_label      0    1      2      3      4      5       6       7       8   \n",
       "\n",
       "alpha_label  TL18ML  ...  BR6MC  BL6MC  BR6ML  BL6ML  BR18MC  BL18MC  BR18ML  \\\n",
       "num_label         9  ...     18     19     20     21      22      23      24   \n",
       "\n",
       "alpha_label  BL18ML  BRArc  BLArc  \n",
       "num_label        25     26     27  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical label of the player, referee, and ball objects (as defined when training the Yolo model):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>alpha_label</th>\n",
       "      <th>player</th>\n",
       "      <th>referee</th>\n",
       "      <th>ball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_label</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "alpha_label  player  referee  ball\n",
       "num_label         0        1     2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe dataframe representation are not used in what follows (original dictionary will be used)\n"
     ]
    }
   ],
   "source": [
    "# Get tactical map keypoints positions dictionary\n",
    "json_path = \"./pitch map labels position.json\"\n",
    "with open(json_path, 'r') as f:\n",
    "    keypoints_map_pos = json.load(f)\n",
    "\n",
    "# Get football field keypoints numerical to alphabetical mapping\n",
    "yaml_path = \"./config pitch dataset.yaml\"\n",
    "with open(yaml_path, 'r') as file:\n",
    "    classes_names_dic = yaml.safe_load(file)\n",
    "classes_names_dic = classes_names_dic['names']\n",
    "\n",
    "# Get football field keypoints numerical to alphabetical mapping\n",
    "yaml_path = \"./config players dataset.yaml\"\n",
    "with open(yaml_path, 'r') as file:\n",
    "    labels_dic = yaml.safe_load(file)\n",
    "labels_dic = labels_dic['names']\n",
    "\n",
    "print(\"Known coordinates of each keypoint on the tactical map:\")\n",
    "display(pd.DataFrame(keypoints_map_pos, index=['x','y']))\n",
    "print(\"Numerical label of field keypoints (as defined when training the Yolo model):\")\n",
    "display(pd.Series(classes_names_dic, name='alpha_label').reset_index().rename({\"index\":\"num_label\"}, axis=1).set_index(\"alpha_label\").transpose())\n",
    "print(\"Numerical label of the player, referee, and ball objects (as defined when training the Yolo model):\")\n",
    "display(pd.Series(labels_dic, name='alpha_label').reset_index().rename({\"index\":\"num_label\"}, axis=1).set_index(\"alpha_label\").transpose())\n",
    "print('\\033[1mThe dataframe representation are not used in what follows (original dictionary will be used)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the video path as weel as the teams colors parameters. We convert the colors to the lab forma in otder to calculate the distances from the team prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set video path\n",
    "video_path = 'test vid.mp4'\n",
    "\n",
    "# Read tactical map image\n",
    "tac_map = cv2.imread('tactical map1.jpg')\n",
    "\n",
    "# Define team colors (based on chosen video)\n",
    "nbr_team_colors = 2\n",
    "colors_dic = {\n",
    "    \"Chelsea\":[(173,216,230), (220,98,88)], # Chelsea colors (Players kit color, GK kit color)\n",
    "    \"Man City\":[(0,0,250), (188,199,3)] # Man City colors (Players kit color, GK kit color)\n",
    "}\n",
    "\n",
    "colors_list = colors_dic[\"Chelsea\"]+colors_dic[\"Man City\"] # Define color list to be used for detected player team prediction\n",
    "color_list_lab = [skimage.color.rgb2lab([i/255 for i in c]) for c in colors_list] # Converting color_list to L*a*b* space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import the YOLOV models. So we have two models for the player detection and for the key points detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 players detection model\n",
    "model_players = YOLO(\"./models/Yolo8L Players/weights/best.pt\")\n",
    "\n",
    "# Load the YOLOv8 field keypoints detection model\n",
    "model_keypoints = YOLO(\"./models/Yolo8M Field Keypoints/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we set some hyperparameters such as the confidence threshold for both models and the key points placement threshold.\n",
    "And then we start the capture loop. this loop, we will process each frame. So here we first increment the frame counter and we read the frame from the capture. So in each frame, we create a new copy of the tactical map so we can display the detected objects coordinates. And then here we set the ball tracking history variable. In case we exceeded a certain number of frames without detecting any ball object.  So the first part in this code is the object detection and coordinate transformation. So here we get the result from both models detections. And we can see we have the results stored here for the bounding boxes for both models as well as the labels. And then we can extract the alphabetical representation of the detected labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 19 players, 1 referee, 1 ball, 3505.7ms\n",
      "Speed: 46.4ms preprocess, 3505.7ms inference, 204.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 1094.0ms\n",
      "Speed: 10.8ms preprocess, 1094.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 players, 1 referee, 1953.1ms\n",
      "Speed: 5.2ms preprocess, 1953.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 863.9ms\n",
      "Speed: 4.2ms preprocess, 863.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 2097.7ms\n",
      "Speed: 5.7ms preprocess, 2097.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 1294.8ms\n",
      "Speed: 4.1ms preprocess, 1294.8ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 players, 1 referee, 1 ball, 1930.8ms\n",
      "Speed: 6.4ms preprocess, 1930.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 918.3ms\n",
      "Speed: 7.2ms preprocess, 918.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 players, 1 referee, 1 ball, 2149.8ms\n",
      "Speed: 4.7ms preprocess, 2149.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 1244.6ms\n",
      "Speed: 7.2ms preprocess, 1244.6ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 players, 1 referee, 1 ball, 1749.7ms\n",
      "Speed: 5.8ms preprocess, 1749.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 1014.4ms\n",
      "Speed: 3.1ms preprocess, 1014.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1670.3ms\n",
      "Speed: 3.7ms preprocess, 1670.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 762.6ms\n",
      "Speed: 4.2ms preprocess, 762.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 2033.4ms\n",
      "Speed: 5.7ms preprocess, 2033.4ms inference, 23.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 BL18MC, 1195.1ms\n",
      "Speed: 32.0ms preprocess, 1195.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1624.0ms\n",
      "Speed: 8.9ms preprocess, 1624.0ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 795.8ms\n",
      "Speed: 3.6ms preprocess, 795.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1427.9ms\n",
      "Speed: 4.7ms preprocess, 1427.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1104.6ms\n",
      "Speed: 4.2ms preprocess, 1104.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1900.7ms\n",
      "Speed: 26.0ms preprocess, 1900.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 938.4ms\n",
      "Speed: 8.4ms preprocess, 938.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1601.9ms\n",
      "Speed: 3.6ms preprocess, 1601.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1039.6ms\n",
      "Speed: 8.0ms preprocess, 1039.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1537.3ms\n",
      "Speed: 5.2ms preprocess, 1537.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 898.3ms\n",
      "Speed: 2.6ms preprocess, 898.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1669.7ms\n",
      "Speed: 3.6ms preprocess, 1669.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 954.5ms\n",
      "Speed: 8.1ms preprocess, 954.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1958.6ms\n",
      "Speed: 9.9ms preprocess, 1958.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1008.7ms\n",
      "Speed: 6.4ms preprocess, 1008.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1698.4ms\n",
      "Speed: 9.5ms preprocess, 1698.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 866.0ms\n",
      "Speed: 4.7ms preprocess, 866.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1403.0ms\n",
      "Speed: 7.2ms preprocess, 1403.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 883.8ms\n",
      "Speed: 4.9ms preprocess, 883.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1833.5ms\n",
      "Speed: 5.7ms preprocess, 1833.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1196.6ms\n",
      "Speed: 5.7ms preprocess, 1196.6ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1807.6ms\n",
      "Speed: 4.2ms preprocess, 1807.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1179.1ms\n",
      "Speed: 14.4ms preprocess, 1179.1ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 2059.8ms\n",
      "Speed: 63.7ms preprocess, 2059.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1 BLArc, 1048.6ms\n",
      "Speed: 10.7ms preprocess, 1048.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1959.8ms\n",
      "Speed: 5.8ms preprocess, 1959.8ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1188.8ms\n",
      "Speed: 4.1ms preprocess, 1188.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 2586.4ms\n",
      "Speed: 5.2ms preprocess, 2586.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1 BLArc, 1007.4ms\n",
      "Speed: 8.5ms preprocess, 1007.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 2571.1ms\n",
      "Speed: 5.9ms preprocess, 2571.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1 BLArc, 1062.8ms\n",
      "Speed: 15.7ms preprocess, 1062.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1972.7ms\n",
      "Speed: 5.8ms preprocess, 1972.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1 BLArc, 1357.6ms\n",
      "Speed: 7.4ms preprocess, 1357.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1929.0ms\n",
      "Speed: 9.8ms preprocess, 1929.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1 BLArc, 1080.1ms\n",
      "Speed: 3.7ms preprocess, 1080.1ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 players, 1 referee, 1 ball, 1786.4ms\n",
      "Speed: 4.7ms preprocess, 1786.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 TLC, 1 TL6MC, 1 TL6ML, 1 TL18MC, 1 TL18ML, 1 TLArc, 1 LMC, 1 BL18MC, 1 BLArc, 1081.5ms\n",
      "Speed: 3.1ms preprocess, 1081.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Process the frame if it was successfuly read\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m     62\u001b[0m     \n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m#################### Part 1 ####################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Run YOLOv8 players inference on the frame\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     results_players \u001b[38;5;241m=\u001b[39m model_players(frame, conf\u001b[38;5;241m=\u001b[39mplayer_model_conf_thresh)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Run YOLOv8 field keypoints inference on the frame\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     results_keypoints \u001b[38;5;241m=\u001b[39m model_keypoints(frame, conf\u001b[38;5;241m=\u001b[39mkeypoints_model_conf_thresh)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:169\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    148\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    149\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:439\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:206\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:285\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 285\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    381\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:46\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[1;32m---> 46\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2[i](x[i]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We set some hyperparameters such as the confidence threshold for both models and the key points placement threshold.\n",
    "\n",
    "\n",
    "# Open video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize frame counter\n",
    "frame_nbr = 0\n",
    "\n",
    "# Set keypoints average displacement tolerance level (in pixels) [set to -1 to always update homography matrix]\n",
    "keypoints_displacement_mean_tol = 10\n",
    "\n",
    "# Set confidence thresholds for players and field keypoints detections\n",
    "player_model_conf_thresh = 0.60\n",
    "keypoints_model_conf_thresh = 0.70\n",
    "\n",
    "# Set variable to record the time when we processed last frame \n",
    "prev_frame_time = 0\n",
    "# Set variable to record the time at which we processed current frame \n",
    "new_frame_time = 0\n",
    "\n",
    "# Store the ball track history\n",
    "ball_track_history = {'src':[],\n",
    "                      'dst':[]\n",
    "}\n",
    "\n",
    "# Count consecutive frames with no ball detected\n",
    "nbr_frames_no_ball = 0\n",
    "# Threshold for number of frames with no ball to reset ball track (frames)\n",
    "nbr_frames_no_ball_thresh = 30\n",
    "# Distance threshold for ball tracking (pixels)\n",
    "ball_track_dist_thresh = 100\n",
    "# Maximum ball track length (detections)\n",
    "max_track_length = 35\n",
    "\n",
    "\n",
    "possesion_times = {\"Chelsea\":0,\"Man City\":0}\n",
    "team_players_coords = {\"Chelsea\":[], \"Man City\":[] }\n",
    "ball_coords = []\n",
    "last_possesion = {\"team\":None,\"player_id\":None} \n",
    "passes = []  # To store pass events\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "\n",
    "    # Update frame counter\n",
    "    frame_nbr += 1\n",
    "\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # Reset tactical map image for each new frame\n",
    "    tac_map_copy = tac_map.copy()\n",
    "\n",
    "    # Reset ball tracks\n",
    "    if nbr_frames_no_ball>nbr_frames_no_ball_thresh:\n",
    "            ball_track_history['dst'] = []\n",
    "            ball_track_history['src'] = []\n",
    "\n",
    "    # Process the frame if it was successfuly read\n",
    "    if success:\n",
    "        \n",
    "        #################### Part 1 ####################\n",
    "        # Object Detection & Coordinate Transofrmation #\n",
    "        ################################################\n",
    "\n",
    "        # Run YOLOv8 players inference on the frame\n",
    "        results_players = model_players(frame, conf=player_model_conf_thresh)\n",
    "        # Run YOLOv8 field keypoints inference on the frame\n",
    "        results_keypoints = model_keypoints(frame, conf=keypoints_model_conf_thresh)\n",
    "\n",
    "        ## Extract detections information\n",
    "        bboxes_p = results_players[0].boxes.xyxy.cpu().numpy()                          # Detected players, referees and ball (x,y,x,y) bounding boxes\n",
    "        bboxes_p_c = results_players[0].boxes.xywh.cpu().numpy()                        # Detected players, referees and ball (x,y,w,h) bounding boxes    \n",
    "        labels_p = list(results_players[0].boxes.cls.cpu().numpy())                     # Detected players, referees and ball labels list\n",
    "        confs_p = list(results_players[0].boxes.conf.cpu().numpy())                     # Detected players, referees and ball confidence level\n",
    "        \n",
    "        bboxes_k = results_keypoints[0].boxes.xyxy.cpu().numpy()                        # Detected field keypoints (x,y,w,h) bounding boxes\n",
    "        bboxes_k_c = results_keypoints[0].boxes.xywh.cpu().numpy()                        # Detected field keypoints (x,y,w,h) bounding boxes\n",
    "        labels_k = list(results_keypoints[0].boxes.cls.cpu().numpy())                   # Detected field keypoints labels list\n",
    "\n",
    "        # Convert detected numerical labels to alphabetical labels\n",
    "        detected_labels = [classes_names_dic[i] for i in labels_k]\n",
    "\n",
    "        # Extract detected field keypoints coordiantes on the current frame\n",
    "        detected_labels_src_pts = np.array([list(np.round(bboxes_k_c[i][:2]).astype(int)) for i in range(bboxes_k_c.shape[0])])\n",
    "\n",
    "        # Get the detected field keypoints coordinates on the tactical map\n",
    "        detected_labels_dst_pts = np.array([keypoints_map_pos[i] for i in detected_labels])\n",
    "\n",
    "\n",
    "        ## Calculate Homography transformation matrix when more than 4 keypoints are detected\n",
    "        if len(detected_labels) > 3:\n",
    "            # Always calculate homography matrix on the first frame\n",
    "            if frame_nbr > 1:\n",
    "                # Determine common detected field keypoints between previous and current frames\n",
    "                common_labels = set(detected_labels_prev) & set(detected_labels)\n",
    "                # When at least 4 common keypoints are detected, determine if they are displaced on average beyond a certain tolerance level\n",
    "                if len(common_labels) > 3:\n",
    "                    common_label_idx_prev = [detected_labels_prev.index(i) for i in common_labels]   # Get labels indexes of common detected keypoints from previous frame\n",
    "                    common_label_idx_curr = [detected_labels.index(i) for i in common_labels]        # Get labels indexes of common detected keypoints from current frame\n",
    "                    coor_common_label_prev = detected_labels_src_pts_prev[common_label_idx_prev]     # Get labels coordiantes of common detected keypoints from previous frame\n",
    "                    coor_common_label_curr = detected_labels_src_pts[common_label_idx_curr]          # Get labels coordiantes of common detected keypoints from current frame\n",
    "                    coor_error = mean_squared_error(coor_common_label_prev, coor_common_label_curr)  # Calculate error between previous and current common keypoints coordinates\n",
    "                    update_homography = coor_error > keypoints_displacement_mean_tol                 # Check if error surpassed the predefined tolerance level\n",
    "                else:\n",
    "                    update_homography = True                                                         \n",
    "            else:\n",
    "                update_homography = True\n",
    "\n",
    "            if  update_homography:\n",
    "                h, mask = cv2.findHomography(detected_labels_src_pts,                   # Calculate homography matrix\n",
    "                                              detected_labels_dst_pts)                  \n",
    "            \n",
    "            detected_labels_prev = detected_labels.copy()                               # Save current detected keypoint labels for next frame\n",
    "            detected_labels_src_pts_prev = detected_labels_src_pts.copy()               # Save current detected keypoint coordiantes for next frame\n",
    "\n",
    "            bboxes_p_c_0 = bboxes_p_c[[i==0 for i in labels_p],:]                       # Get bounding boxes information (x,y,w,h) of detected players (label 0)\n",
    "            bboxes_p_c_2 = bboxes_p_c[[i==2 for i in labels_p],:]                       # Get bounding boxes information (x,y,w,h) of detected ball(s) (label 2)\n",
    "\n",
    "            # Get coordinates of detected players on frame (x_cencter, y_center+h/2)\n",
    "            detected_ppos_src_pts = bboxes_p_c_0[:,:2]  + np.array([[0]*bboxes_p_c_0.shape[0], bboxes_p_c_0[:,3]/2]).transpose()\n",
    "            # Get coordinates of the first detected ball (x_center, y_center)\n",
    "            detected_ball_src_pos = bboxes_p_c_2[0,:2] if bboxes_p_c_2.shape[0]>0 else None\n",
    "\n",
    "            # Transform players coordinates from frame plane to tactical map plance using the calculated Homography matrix\n",
    "            pred_dst_pts = []                                                           # Initialize players tactical map coordiantes list\n",
    "            for pt in detected_ppos_src_pts:                                            # Loop over players frame coordiantes\n",
    "                pt = np.append(np.array(pt), np.array([1]), axis=0)                     # Covert to homogeneous coordiantes\n",
    "                dest_point = np.matmul(h, np.transpose(pt))                             # Apply homography transofrmation\n",
    "                dest_point = dest_point/dest_point[2]                                   # Revert to 2D-coordiantes\n",
    "                pred_dst_pts.append(list(np.transpose(dest_point)[:2]))                 # Update players tactical map coordiantes list\n",
    "            pred_dst_pts = np.array(pred_dst_pts)\n",
    "            \n",
    "        \n",
    "            # Transform ball coordinates from frame plane to tactical map plane using the calculated Homography matrix\n",
    "            if detected_ball_src_pos is not None:\n",
    "                pt = np.append(np.array(detected_ball_src_pos), np.array([1]), axis=0)\n",
    "                dest_point = np.matmul(h, np.transpose(pt))\n",
    "                dest_point = dest_point/dest_point[2]\n",
    "                detected_ball_dst_pos = np.transpose(dest_point)\n",
    "\n",
    "                # Update track ball position history\n",
    "                if len(ball_track_history['src'])>0 :\n",
    "                    if np.linalg.norm(detected_ball_src_pos-ball_track_history['src'][-1])<ball_track_dist_thresh:\n",
    "                        ball_track_history['src'].append((int(detected_ball_src_pos[0]), int(detected_ball_src_pos[1])))\n",
    "                        ball_track_history['dst'].append((int(detected_ball_dst_pos[0]), int(detected_ball_dst_pos[1])))\n",
    "                    else:\n",
    "                        ball_track_history['src']=[(int(detected_ball_src_pos[0]), int(detected_ball_src_pos[1]))]\n",
    "                        ball_track_history['dst']=[(int(detected_ball_dst_pos[0]), int(detected_ball_dst_pos[1]))]\n",
    "                else:\n",
    "                    ball_track_history['src'].append((int(detected_ball_src_pos[0]), int(detected_ball_src_pos[1])))\n",
    "                    ball_track_history['dst'].append((int(detected_ball_dst_pos[0]), int(detected_ball_dst_pos[1])))\n",
    "            # Remove oldest tracked ball postion if track exceedes threshold        \n",
    "            if len(ball_track_history) > max_track_length:\n",
    "                    ball_track_history['src'].pop(0)\n",
    "                    ball_track_history['dst'].pop(0)\n",
    "                    \n",
    "     \n",
    "        ######### Part 2 ##################################\n",
    "        # Players Team Prediction and Pass Recognizing System \n",
    "        ###################################################\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                                      # Convert frame to RGB\n",
    "        obj_palette_list = []                                                                   # Initialize players color palette list\n",
    "        palette_interval = (0,5)                                                                # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "        annotated_frame = frame                                                                 # Create annotated frame \n",
    "\n",
    "        ## Loop over detected players (label 0) and extract dominant colors palette based on defined interval\n",
    "        for i, j in enumerate(list(results_players[0].boxes.cls.cpu().numpy())):\n",
    "            if int(j) == 0:\n",
    "                bbox = results_players[0].boxes.xyxy.cpu().numpy()[i,:]                         # Get bbox info (x,y,x,y)\n",
    "                obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "                obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "                center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "                center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "                center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "                center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "                center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                                        center_filter_x1:center_filter_x2]\n",
    "                obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "                    \n",
    "                reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "                palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "                palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "                color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "                RGB_df = pd.DataFrame(color_count, columns = ['cnt', 'RGB']).sort_values(       # Create dataframe based on defined palette interval\n",
    "                                      by = 'cnt', ascending = False).iloc[\n",
    "                                          palette_interval[0]:palette_interval[1],:]\n",
    "                palette = list(RGB_df.RGB)                                                      # Convert palette to list (for faster processing)\n",
    "                annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "                                                (int(bbox[0])+center_filter_x1, \n",
    "                                                 int(bbox[1])+ center_filter_y1),  \n",
    "                                                (int(bbox[0])+center_filter_x2, \n",
    "                                                 int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "                \n",
    "                # Update detected players color palette list\n",
    "                obj_palette_list.append(palette)\n",
    "        \n",
    "        ## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "        players_distance_features = []\n",
    "        # Loop over detected players extracted color palettes\n",
    "        for palette in obj_palette_list:\n",
    "            palette_distance = []\n",
    "            palette_lab = [skimage.color.rgb2lab([i/255 for i in color]) for color in palette]  # Convert colors to L*a*b* space\n",
    "            # Loop over colors in palette\n",
    "            for color in palette_lab:\n",
    "                distance_list = []\n",
    "                # Loop over predefined list of teams colors\n",
    "                for c in color_list_lab:\n",
    "                    #distance = np.linalg.norm([i/255 - j/255 for i,j in zip(color,c)])\n",
    "                    distance = skimage.color.deltaE_cie76(color, c)                             # Calculate Euclidean distance in Lab color space\n",
    "                    distance_list.append(distance)                                              # Update distance list for current color\n",
    "                palette_distance.append(distance_list)                                          # Update distance list for current palette\n",
    "            players_distance_features.append(palette_distance)                                  # Update distance features list\n",
    "\n",
    "        ## Predict detected players teams based on distance features\n",
    "        players_teams_list = []\n",
    "        # Loop over players distance features\n",
    "        for distance_feats in players_distance_features:\n",
    "            vote_list=[]\n",
    "            # Loop over distances for each color \n",
    "            for dist_list in distance_feats:\n",
    "                team_idx = dist_list.index(min(dist_list))//nbr_team_colors                     # Assign team index for current color based on min distance\n",
    "                vote_list.append(team_idx)                                                      # Update vote voting list with current color team prediction\n",
    "            players_teams_list.append(max(vote_list, key=vote_list.count))                      # Predict current player team by vote counting\n",
    "            \n",
    "    # Append player coordinates to team_player_coords\n",
    "        for idx, team_idx in enumerate(players_teams_list):\n",
    "            team_name = list(colors_dic.keys())[team_idx]  # Retrieve team name based on index\n",
    "            player_coord = {\"frame\": frame_nbr, \"x\": pred_dst_pts[idx][0], \"y\": pred_dst_pts[idx][1]}\n",
    "            team_players_coords[team_name].append(player_coord)\n",
    "            \n",
    "    # If ball detected, append its coordinates\n",
    "        if detected_ball_src_pos is not None:\n",
    "            ball_coord = {\"frame\": frame_nbr, \"x\": int(detected_ball_dst_pos[0]), \"y\": int(detected_ball_dst_pos[1])}\n",
    "            ball_coords.append(ball_coord) \n",
    "    \n",
    "    # If the ball position was updated in this frame\n",
    "        if detected_ball_src_pos is not None:\n",
    "            closest_distance = float('inf')\n",
    "            ball_position = np.array([ball_coords[-1]['x'], ball_coords[-1]['y']])\n",
    "            for team_name, players in team_players_coords.items(): \n",
    "                for player_id, player_coord in enumerate(players[-1:]):                                     # Use only the most recent player positions\n",
    "                    player_position = np.array([player_coord['x'], player_coord['y']])\n",
    "                    distance = np.linalg.norm(ball_position - player_position)\n",
    "                    if distance < closest_distance:        \n",
    "                        closest_distance = distance\n",
    "                        \n",
    "                        # Update the ball's last possession details\n",
    "                    last_possession = {\"team\": team_name, \"player_id\": player_id, \"distance\": closest_distance}\n",
    "\n",
    "                        \n",
    "        # Assuming we have a previous frame to compare with\n",
    "        if len(ball_coords) > 1 and 'closest_player' in ball_coords[-2]:\n",
    "            prev_closest_player = ball_coords[-2]['closest_player']\n",
    "            current_closest_player = last_possession\n",
    "            # Check if possession changed (indicating a pass)\n",
    "            if prev_closest_player['player_id'] != current_closest_player['player_id']:\n",
    "                # Record a pass event\n",
    "                pass_event = {\n",
    "                \"from_team\": prev_closest_player[\"team\"],\n",
    "                \"from_player\": prev_closest_player[\"player_id\"],\n",
    "                \"to_team\": current_closest_player[\"team\"],\n",
    "                \"to_player\": current_closest_player[\"player_id\"],\n",
    "                \"frame\": ball_coords[-1][\"frame\"]\n",
    "                             }\n",
    "                passes.append(pass_event)\n",
    "            \n",
    "                # Update the ball_coords with the closest player info for the current frame\n",
    "                ball_coords[-1].update({\"closest_player\": last_possession})\n",
    "\n",
    "\n",
    "        #################### Part 3 #####################\n",
    "        # Updated Frame & Tactical Map With Annotations #\n",
    "        #################################################\n",
    "\n",
    "        ball_color_bgr = (0,0,255)                                                          # Color (GBR) for ball annotation on tactical map\n",
    "        j=0                                                                                                 # Initializing counter of detected players\n",
    "        palette_box_size = 10                                                                               # Set color box size in pixels (for display)\n",
    "       \n",
    "\n",
    "        # Loop over all detected object by players detection model\n",
    "        for i in range(bboxes_p.shape[0]):\n",
    "            conf = confs_p[i]                                                                               # Get confidence of current detected object\n",
    "            if labels_p[i]==0:                                                                              # Display annotation for detected players (label 0)\n",
    "                \n",
    "                # Display extracted color palette for each detected player\n",
    "                palette = obj_palette_list[j]                                                               # Get color palette of the detected player\n",
    "                for k, c in enumerate(palette):\n",
    "                    c_bgr = c[::-1]                                                                         # Convert color to BGR\n",
    "                    annotated_frame = cv2.rectangle(annotated_frame, (int(bboxes_p[i,2])+3,                 # Add color palette annotation on frame\n",
    "                                                            int(bboxes_p[i,1])+k*palette_box_size),\n",
    "                                                            (int(bboxes_p[i,2])+palette_box_size,\n",
    "                                                            int(bboxes_p[i,1])+(palette_box_size)*(k+1)),\n",
    "                                                              c_bgr, -1)\n",
    "\n",
    "                team_name = list(colors_dic.keys())[players_teams_list[j]]                                  # Get detected player team prediction\n",
    "                color_rgb = colors_dic[team_name][0]                                                        # Get detected player team color\n",
    "                color_bgr = color_rgb[::-1]                                                                 # Convert color to bgr\n",
    "\n",
    "                annotated_frame = cv2.rectangle(annotated_frame, (int(bboxes_p[i,0]), int(bboxes_p[i,1])),  # Add bbox annotations with team colors\n",
    "                                                (int(bboxes_p[i,2]), int(bboxes_p[i,3])), color_bgr, 1)\n",
    "                \n",
    "                cv2.putText(annotated_frame, team_name + f\" {conf:.2f}\",                                    # Add team name annotations\n",
    "                             (int(bboxes_p[i,0]), int(bboxes_p[i,1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                               color_bgr, 2)\n",
    "                \n",
    "                # Add tactical map player postion color coded annotation if more than 3 field keypoints are detected\n",
    "                if len(detected_labels_src_pts)>3:\n",
    "                    tac_map_copy = cv2.circle(tac_map_copy, (int(pred_dst_pts[j][0]),int(pred_dst_pts[j][1])),\n",
    "                                          radius=5, color=color_bgr, thickness=-1)\n",
    "\n",
    "                j+=1                                                                                        # Update players counter\n",
    "            else:                                                                                           # Display annotation for otehr detections (label 1, 2)\n",
    "                annotated_frame = cv2.rectangle(annotated_frame, (int(bboxes_p[i,0]), int(bboxes_p[i,1])),  # Add white colored bbox annotations\n",
    "                                                 (int(bboxes_p[i,2]), int(bboxes_p[i,3])), (255,255,255), 1)\n",
    "                cv2.putText(annotated_frame, labels_dic[labels_p[i]] + f\" {conf:.2f}\",                      # Add white colored label text annotations\n",
    "                            (int(bboxes_p[i,0]), int(bboxes_p[i,1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                              (255,255,255), 2)\n",
    "\n",
    "                # Add tactical map ball postion annotation if detected\n",
    "                if detected_ball_src_pos is not None:\n",
    "                    tac_map_copy = cv2.circle(tac_map_copy, (int(detected_ball_dst_pos[0]), \n",
    "                                                   int(detected_ball_dst_pos[1])), radius=5, \n",
    "                                                   color=ball_color_bgr, thickness=3)\n",
    "        for i in range(bboxes_k.shape[0]):\n",
    "            annotated_frame = cv2.rectangle(annotated_frame, (int(bboxes_k[i,0]), int(bboxes_k[i,1])),  # Add bbox annotations with team colors\n",
    "                                        (int(bboxes_k[i,2]), int(bboxes_k[i,3])), (0,0,0), 1)\n",
    "        \n",
    "        # Plot the ball tracks on tactical map\n",
    "        if len(ball_track_history['src'])>0:\n",
    "            points = np.hstack(ball_track_history['dst']).astype(np.int32).reshape((-1, 1, 2))\n",
    "            tac_map_copy = cv2.polylines(tac_map_copy, [points], isClosed=False, color=(0, 0, 100), thickness=2)\n",
    "        \n",
    "        # Combine annotated frame and tactical map in one image with colored border separation\n",
    "        border_color = [255,255,255]                                                                        # Set border color (BGR)\n",
    "        annotated_frame=cv2.copyMakeBorder(annotated_frame, 40, 10, 10, 10,                                 # Add borders to annotated frame\n",
    "                                            cv2.BORDER_CONSTANT, value=border_color)\n",
    "        tac_map_copy = cv2.copyMakeBorder(tac_map_copy, 70, 50, 10, 10, cv2.BORDER_CONSTANT,                # Add borders to tactical map \n",
    "                                           value=border_color)      \n",
    "        tac_map_copy = cv2.resize(tac_map_copy, (tac_map_copy.shape[1], annotated_frame.shape[0]))          # Resize tactical map\n",
    "        final_img = cv2.hconcat((annotated_frame, tac_map_copy))                                            # Concatenate both images\n",
    "        ## Add info annotation\n",
    "        cv2.putText(final_img, \"Tactical Map\", (1370,60), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2)\n",
    "        cv2.putText(final_img, \"Press 'p' to pause & 'q' to quit\", (820,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2)\n",
    "\n",
    "        new_frame_time = time.time()                                                                        # Get time after finished processing current frame\n",
    "        fps = 1/(new_frame_time-prev_frame_time)                                                            # Calculate FPS as 1/(frame proceesing duration)\n",
    "        prev_frame_time = new_frame_time                                                                    # Save current time to be used in next frame\n",
    "        cv2.putText(final_img, \"FPS: \" + str(int(fps)), (20,30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 2)\n",
    "        \n",
    "        # Display the final annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Players and Field Keypoints Detection with Team Prediction and Tactical Map\",    \n",
    "                    final_img)\n",
    "\n",
    "        # Treat keyboard user inputs (\"p\" for pause/unpause & \"q\" for quit)\n",
    "        key = cv2.waitKey(1)\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        if key == ord('p'):\n",
    "            cv2.waitKey(-1) #wait until any key is pressed\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "################# Part 4 #########################\n",
    "# Save Coordinates and pass event into a json and csv file\n",
    "##################################################\n",
    "\n",
    "with open('team_player_coords.json', 'w') as json_file:\n",
    "    json.dump(team_players_coords, json_file)\n",
    "\n",
    "with open('ball_coords.json', 'w') as json_file:\n",
    "    json.dump(ball_coords, json_file)\n",
    "  \n",
    "# Save pass events to JSON \n",
    "\n",
    "with open('pass_events.json', 'w') as json_file:\n",
    "    json.dump(passes, json_file)\n",
    "    \n",
    "    \n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DetectionYolo8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
